Artificial intelligence was born in the 1950s, when a handful of pioneers from the
nascent field of computer science started asking whether computers could be made to
“think”—a question whose ramifications we’re still exploring today. 
A concise definition of the field would be as follows: the effort to automate intellectual tasks normally performed by humans. 
As such, AI is a general field that encompasses machine learning and
deep learning, but that also includes many more approaches that don’t involve any
learning. Early chess programs, for instance, only involved hardcoded rules crafted by
programmers, and didn’t qualify as machine learning. For a fairly long time, many
experts believed that human-level artificial intelligence could be achieved by having
programmers handcraft a sufficiently large set of explicit rules for manipulating knowledge
This approach is known as symbolic AI, and it was the dominant paradigm in AI from the 1950s to the late 1980s
It reached its peak popularity during the expert systems boom of the 1980s.
Although symbolic AI proved suitable to solve well-defined, logical problems, such as
playing chess, it turned out to be intractable to figure out explicit rules for solving more
complex, fuzzy problems, such as image classification, speech recognition, and language translation.